# Searching with Gravwell

Gravwell is a structure-on-read data lake and the search pipeline is the core of that functionality. This is where the power and flexibility of the platform can be seen as information about data doesn't need to be known before collection. Instead, we can ingest the raw data and assemble a search processing pipeline to turn that raw data into actionable results! This all but eliminates normalizing or massaging data to try and fit specific molds, a common issue with SIEMs. Further, it let's us perform analysis retroactively against older data when new information or techniques become available.

The search pipeline is the core of Gravwell's functionality and operates in a similar fashion to the Linux/Unix command line. The Gravwell search query syntax assembles a processing pipeline to search and transform raw data into results. The pipeline modules often consist of at least one extraction module, one or more filtering or processing modules, and a rendering module. For example, the following query fetches data tagged "reddit", uses the json module to extract a field called Body, then uses the eval module to filter out any entries with Body fields longer than 20 characters, then finally uses the table renderer to display the contents of the Body fields:

```
tag=reddit json Body | eval len(Body) < 20 | table Body
```

The user interface used to run queries [is documented here](#!gui/queries/queries.md).

## Components of a Query

The example query above can be broken into several individual portions, which are described below.

### Tag Specification

The first part of a Gravwell query is the tag specification. The user must specify one or more tags from which the indexers should pull data to put into the start of the pipeline. The following tells Gravwell to take data tagged "reddit":

```
tag=reddit
```

You can also specify multiple tags:

```
tag=reddit,hackernews
```

Or you can use wildcards:

```
tag=syslog-http-*,syslog-firewall-*
```

### Extraction Modules

Extraction modules are used to pull out fields/values/properties from the raw underlying data. 

```
tag=reddit json Body | eval len(Body) < 20 | table Body
```

In the example above, the `JSON` module is extracting the values from the raw JSON text and providing them as "Enumerated Values" for any modules further down the pipeline. In other words, this module parses the JSON and provides the data as a structure. In the `json Body` example, we are extracting a single value from the raw JSON text.

The full documentation for these modules can be found at [Extration Modules](extractionmodules.md)

### Search Modules

Search modules are used to analyze data entries, filtering out undesired data or extracting interesting portions of the data. A search pipeline may include many search modules, one after another, each operating on the results of the previous module.

In the example above, `eval` is a search module performing some processing and filtering. Note that many search modules can be made to operate on a subset of the overall query tags by putting an *additional* tag specification before the module invocation -- something we call Data Fusion. This can be very useful if you need to perform some initial extractions differently:

```
tag=reddit,hackernews tag=reddit json Body | tag=hackernews json body as Body | eval len(Body) < 20 | table Body
```

[Click here for complete documentation of search modules](searchmodules.md)


### Render Modules

Render modules take the results generated by the search modules and present them to the user graphically. A search pipeline will only include one render module, at the very end of the pipeline.

```
tag=reddit json Body | eval len(Body) < 20 | table Body
```

In the example above, "table" is the render module; it has been told to display the contents of the Body Enumerated Value in a single column.


[Click here for complete documentation of render modules](rendermodules.md)


## Enumerated Values

Enumerated values are special data elements which are created and used within the search pipeline. In the pipeline below, several enumerated values are created.

```
tag=reddit json Body | langfind -e Body | count by lang | sort by count desc | table lang count
```

First, the json module parses the JSON in the raw entries and pulls out the "Body" element, storing it in an enumerated value named `Body`. Then the langfind module accesses the `Body` enumerated values and attempts to analyze the language used; it puts the result in a new enumerated value called `lang`. Next, the count module reads the `lang` enumerated values and counts how many times each value appears, storing the results in enumerated values named `count`. The rest of the pipeline sorts the results based on the counts and creates a table from the `lang` and `count` enumerated values.

Further examples throughout the documentation should help clarify the use of enumerated values.

## Filtering

Gravwell extraction modules will typically allow *extracted* items to be *filtered* at extraction time. Filtering lets you drop or pass entries based on whether or not they match particular criteria; see [this page](filtering.md) for full documentation.

## Quoting and tokenizing

When specifying arguments to Gravwell modules, be mindful of special characters. Most modules treat spaces, tabs, newlines, and the following characters as separators: !#$%&'()*+,-./:;<=>?@

When specifying an argument to a module which contains one of these characters, wrap the argument in double-quotes:

```
json "search-id"
```

```
grep "dank memes"
```

You can escape double quote characters if you need to use them, for instance to identify mis-used dialog tags in text you could search for the sequence `",`:

```
grep "\","
```

## Macros

Macros can help turn long, complex queries into easy-to-remember shortcuts. See [the full macro documentation](#!search/macros.md) for more information.

## Compound Queries

You can combine multiple queries together as a single "compound" query in order to leverage multiple data sources, fuse data into another query, and simplify complex queries. Gravwell's compound query syntax is a simple sequence of in-order queries, with additional notation to create temporary resources that can be referenced in queries later in the sequence. 

A compound query consists of a main query (the last query in a sequence), and one or more subqueries. The main query is written just like a normal query, while subqueries are always wrapped in the notation `@<identifier>{<query>}`. Queries are separated by `;`. 

Subqueries generate named resources in the form of `@<identifier>`. These resources can be used as regular resources with any module that supports resources such as `lookup`, `ipexist`, ... Named resources are scoped to the compound query they exist in, and are ephemeral - they are only accessible to other queries in the compound query, and are deleted as soon as the query is completed. 

For example, say we chart across the last week of DNS records, grouping our output in 10 minute bins, using the query:

```
tag=dns count over 10m | chart count
```

This produces a chart:

![](compound-ex1.png)

Notice that several spikes of DNS activity are present. Now say we wish to only see the records present for spikes with a count greater than 3000. Normally this wouldn't be possible using a single query as the filter (count > 3000) is itself a function of the dataset and would require computation across the entire dataset first. Compound queries enable this kind of query by allowing us to start with a subquery that creates a temporary table of counts, and a main query that filters on the count, while preserving the original data.

Let's start with the subquery:

```
tag=dns time -f "Mon Jan _2 15:0" TIMESTAMP ts | count over 10m | table ts count
```

In the subquery, we've added a time module in the pipeline to enumerate timestamps in 10 minute bins as the enumerated value `ts`. We'll use the `ts` enumerated value later to index in this table.

Since this is a subquery, we need to give it a name so later queries can reference its output, and wrap the query in braces. We'll call this subquery "counts":

```
@counts{tag=dns time -f "Mon Jan _2 15:0" TIMESTAMP ts | count over 10m | table ts count}
```

In the main query, we again use the DNS dataset, and use the `lookup` module to read from our subquery "counts":

```
tag=dns time -f "Mon Jan _2 15:0" TIMESTAMP ts | lookup -r @counts ts ts count | eval count > 3000 | chart
```

This query again formats our 10 minute window timestamp "ts", which we use to match into the table "@counts" generated by our subquery. We then simply filter, using eval, any entry belonging to a bin with a count greater than 3000. 

We wrap this into a compound query simply by joining the queries together and separating them with a `;`:

```
@counts{
	tag=dns time -f "Mon Jan _2 15:0" TIMESTAMP ts 
	| count over 10m 
	| table ts count
};

tag=dns time -f "Mon Jan _2 15:0" TIMESTAMP ts 
| lookup -r @counts ts ts count 
| eval count > 3000 
| chart
```

This gives us a chart with just the large spikes of data passed through:

![](compound-ex2.png)

It's also possible to have any number of subqueries in a compound query, as in:

```
@q1{...}; @q2{...}; @q3{...}; ...
```

## Comments

Queries support C-Style comments anywhere in the query text. Comments are saved in the search histroy, and are useful for debugging queries and adding inline notes. For example:

```
tag=foo json foo.bar /* a c-style comment that has no impact on the search */ baz | table
```

